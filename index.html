
<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

    <title>Jiannan Xiang</title>

    <meta name="author" content="Jiannan Xiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
<script type="text/javascript" src="jquery-1.12.4.min.js"></script></head>

<body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:0px">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.7%;width:75%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Jiannan Xiang</name>
                                    </p>
                                    <p>
                                        I am a second-year PhD student at University of California, San Diego, under the supervision of Prof. <a href="http://zhiting.ucsd.edu/" target="_blank">Zhiting Hu</a>. Previously, I got my Master's degree from Machine Learning Department at Carnegie Mellon University. My research interests lie in building general world models and leveraging them to advance reasoning and planning in real-world tasks.
                                    </p>
                                    <p>
                                        Email: jixiang[at]ucsd.edu
                                    </p>
                                    <p style="text-align:left">
                                        <a href="https://github.com/szxiangjn" target="_blank">
                                        Github</a> &nbsp;&nbsp;/&nbsp;&nbsp;
                                        <a href="https://scholar.google.com/citations?user=l8BS2wsAAAAJ&hl=en" target="_blank">
                                        Google Scholar</a> &nbsp;&nbsp;/&nbsp;&nbsp; 
                                        <a href="https://twitter.com/szxiangjn" target="_blank">
                                        Twitter</a> &nbsp;&nbsp;/&nbsp;&nbsp;
                                        <a href="https://www.linkedin.com/in/jiannan-xiang-0689281b7/" target="_blank">
                                        LinkedIn</a> &nbsp;&nbsp;&nbsp;&nbsp;
                                    </p>
                                </td>
                                <td style="padding:2%;width:40%;max-width:40%">
                                    <a><img style="width:120%;max-width:120%" alt="profile photo" src="photo.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Selected Publications</heading> (* equal contribution)
                                    <p>
                                        <a href="https://arxiv.org/abs/2505.12705" target="_blank"><papertitle>DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories</papertitle></a>
                                        <br>
                                        Joel Jang*, Seonghyeon Ye*, Zongyu Lin*, <strong>Jiannan Xiang*</strong>, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xiaohui Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, Linxi Fan
                                        <br>
                                        <em>Preprint</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2505.12705" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://research.nvidia.com/labs/gear/dreamgen/" target="_blank">blog</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://openreview.net/pdf/fad1cdbf1a8687d3e2a1924573c79977176f6b06.pdf" target="_blank"><papertitle>Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</papertitle></a>
                                        <br>
                                        Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, <strong>Jiannan Xiang</strong>, Benhao Huang, David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu
                                        <br>
                                        <em>ACL 2025, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://openreview.net/pdf/fad1cdbf1a8687d3e2a1924573c79977176f6b06.pdf" target="_blank">paper</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://research.nvidia.com/publication/2025-03_nvidia-isaac-gr00t-n1-open-foundation-model-humanoid-robots" target="_blank"><papertitle>GR00T N1: An Open Foundation Model for Generalist Humanoid Robots</papertitle></a>
                                        <br>
                                        <strong>NVIDIA (Core Contributor)</strong>&nbsp;
                                        <br>
                                        <em>Whitepaper</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2503.14734" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://developer.nvidia.com/blog/accelerate-generalist-humanoid-robot-development-with-nvidia-isaac-gr00t-n1/" target="_blank">blog</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/NVIDIA/Isaac-GR00T" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://world-model.maitrix.org/assets/pandora.pdf" target="_blank"><papertitle>Pandora: Towards General World Model with Natural Language Actions and Video States</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang</strong>, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, Zhengzhong Liu, Eric P. Xing, Zhiting Hu
                                        <br>
                                        <em>Technical Report</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://world-model.maitrix.org/assets/pandora.pdf" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://world-model.ai" target="_blank">website</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/maitrix-org/Pandora" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2401.08743" target="_blank"><papertitle>MMToM-QA: Multimodal Theory of Mind Question Answering</papertitle></a>
                                        <br>
                                        Chuanyang Jin, Yutong Wu, Jing Cao, <strong>Jiannan Xiang</strong>, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B Tenenbaum, Tianmin Shu
                                        <br>
                                        <em>ACL 2024</em> <span style="color: red;">(Outstanding Paper Award)</span>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2401.08743" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/chuanyangjin/MMToM-QA" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2305.10626" target="_blank"><papertitle>Language Models Meet World Models: Embodied Experiences Enhance Language Models</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang*</strong>, Tianhua Tao*, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu
                                        <br>
                                        <em>NeurIPS 2023</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2305.10626" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/szxiangjn/world-model-for-language-model" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <!-- <p>
                                        <a href="https://arxiv.org/abs/2210.04325" target="_blank"><papertitle>ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang</strong>,&nbsp;
                                        <a href="https://hunterhector.github.io/" target="_blank">Zhengzhong Liu</a>,&nbsp;
                                        Yucheng Zhou,&nbsp;
                                        <a href="http://www.cs.cmu.edu/~epxing/" target="_blank">Eric P. Xing</a>,&nbsp;
                                        <a href="http://zhiting.ucsd.edu/" target="_blank">Zhiting Hu</a>&nbsp;
                                        <br>
                                        <em>EMNLP 2022, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2210.04325" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/szxiangjn/any-shot-data2text" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2203.15858" target="_blank"><papertitle>Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang</strong>,&nbsp;
                                        <a href="https://sites.google.com/view/huayangli" target="_blank">Huayang Li</a>,&nbsp;
                                        <a href="https://yhlleo.github.io/" target="_blank">Yahui Liu</a>,&nbsp;
                                        <a href="https://lemaoliu.github.io/homepage/" target="_blank">Lemao Liu</a>,&nbsp;
                                        <a href="https://scholar.google.com/citations?user=xSkkA7UAAAAJ&hl=en&oi=ao" target="_blank">Guoping Huang</a>,&nbsp;
                                        <a href="http://staff.ustc.edu.cn/~liandefu/" target="_blank">Defu Lian</a>,&nbsp;
                                        <a href="https://www.linkedin.com/in/shumingshi/?originalSubdomain=cn" target="_blank">Shuming Shi</a>&nbsp;
                                        <br>
                                        <em>ACL 2022, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2203.15858" target="_blank">paper</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2203.15860" target="_blank"><papertitle>Visualizing the Relationship Between Encoded Linguistic Information and Task Performance</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang*</strong>,&nbsp;
                                        <a href="https://sites.google.com/view/huayangli" target="_blank">Huayang Li*</a>,&nbsp;
                                        <a href="http://staff.ustc.edu.cn/~liandefu/" target="_blank">Defu Lian</a>,&nbsp;
                                        <a href="https://scholar.google.com/citations?user=xSkkA7UAAAAJ&hl=en&oi=ao" target="_blank">Guoping Huang</a>,&nbsp;
                                        <a href="https://sites.google.com/site/tarowtnb/" target="_blank">Taro Watanabe</a>,&nbsp;
                                        <a href="https://lemaoliu.github.io/homepage/" target="_blank">Lemao Liu</a>&nbsp;
                                        <br>
                                        <em>ACL 2022, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2203.15860" target="_blank">paper</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2105.02573" target="_blank"><papertitle>Assessing Dialogue Systems with Distribution Distances</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang*</strong>,&nbsp;
                                        <a href="https://yhlleo.github.io/" target="_blank">Yahui Liu*</a>,&nbsp;
                                        <a href="https://jcyk.github.io/" target="_blank">Deng Cai</a>,&nbsp;
                                        <a href="https://sites.google.com/view/huayangli" target="_blank">Huayang Li</a>,&nbsp;
                                        <a href="http://staff.ustc.edu.cn/~liandefu/" target="_blank">Defu Lian</a>,&nbsp;
                                        <a href="https://lemaoliu.github.io/homepage/" target="_blank">Lemao Liu</a>&nbsp;
                                        <br>
                                        <em>ACL 2021, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2105.02573" target="_blank">paper</a>&nbsp;&nbsp;/&nbsp;&nbsp;<a href="https://github.com/yhlleo/frechet-bert-distance" target="_blank">code</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://arxiv.org/abs/2009.13112" target="_blank"><papertitle>Learning to stop: A Simple yet Effective Approach to Urban Vision-Language Navigation</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang</strong>,&nbsp;
                                        <a href="https://eric-xw.github.io/" target="_blank">Xin Eric Wang</a>,&nbsp;
                                        <a href="https://sites.cs.ucsb.edu/~william/" target="_blank">William Yang Wang</a>&nbsp;
                                        <br>
                                        <em>EMNLP 2020, Findings</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://arxiv.org/abs/2009.13112" target="_blank">paper</a>&nbsp;&nbsp;
                                        <br>
                                    </p>
                                    <p>
                                        <a href="https://vigilworkshop.github.io/static/papers-2019/5.pdf" target="_blank"><papertitle>Not All Actions Are Equal: Learning to Stop in Language-Grounded Urban Navigation</papertitle></a>
                                        <br>
                                        <strong>Jiannan Xiang</strong>, 
                                        Xin Eric Wang, 
                                        William Yang Wang
                                        <br>
                                        <em>NeurIPS 2019, ViGIL workshop</em>&nbsp;&nbsp;
                                        <br>
                                        <a href="https://vigilworkshop.github.io/static/papers-2019/5.pdf" target="_blank">paper</a>&nbsp;&nbsp;
                                        <br>
                                    </p> -->
                                </td>
                            </tr>      
                        </tbody>
                    </table>
                    <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Services</heading>
                                    <p>
                                        <li class="paper"> Reviewers: NeurIPS 2023, ACL 2023, AAAI 2023, EMNLP 2022, ACL ARR 2022, ACL ARR 2021.
                                        </li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table> -->
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <br>
                                    <div>
                                        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=HSpqMyQrjFG9LeWo3zlULzY-wpHMPU_BO8Vr-_VcO1M&cmn=3acc3a&cmo=ff5353&co=2d78ad&ct=ffffff'></script>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <p font-size:small;="">
                                        <br>
                                        <br>
                                        </p><div style="float:left;">
                                            Updated at Aug. 2025
                                        </div>
                                        <div style="float:right;">
                                            Thanks <a href="https://jonbarron.info/">Jon Barron</a> for this amazing work
                                        </div>
                                        <br>
                                        <br>        
                                    <p></p>                           
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </tbody></table>

<div class="jvectormap-tip"></div></body></html>
